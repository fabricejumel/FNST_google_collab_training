{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabricejumel/FNST_google_collab_training/blob/main/fast_neural_style_transfer_DRIVE_to_CORRECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDrgigbODaxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyDkvvhIV9Kl"
      },
      "source": [
        "# Neural Style Transfer\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/ocl42wcx9c6y5w1/animate.gif?raw=1\" width=\"900\" align = \"center\"/>\n",
        "<center>Fig 1. Neural Style Transfer</center>\n",
        "<br>\n",
        "\n",
        "According to Wikipedia, Neural Style Transfer (NST) also called Artistic Style Transfer refers to a class of software algorithms that manipulate digital images, or videos, in order to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks for the sake of image transformation.\n",
        "<br>\n",
        "\n",
        "***In simple words, Neural style transfer is the process of creating art using computers. It is the process of painting the contents of one image with the style of another.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVqh9m83V9Kp"
      },
      "source": [
        "## Content Image\n",
        "This is our content inspiration for final output image. The contents of final image will be similar to this.\n",
        "<br>\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/rpymcavlpb5iqo4/japanese_garden.jpg?raw=1\" width=\"500\" align = \"center\"/>\n",
        "<center>Fig 2. Content Image</center>\n",
        "<br>\n",
        "\n",
        "## Style Image\n",
        "This is our style inspiration for our final output image. The style of final image will be similar to this.\n",
        "<br>\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/daqk93nltejhwfi/picasso_selfportrait.jpg?raw=1\" width=\"300\" align = \"center\"/>\n",
        "<center>Fig 3. Style Image</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwtEsIirV9Kq"
      },
      "source": [
        "## **How does NST work?**\n",
        "<img src=\"https://miro.medium.com/max/1294/1*ZgW520SZr1QkGoFd3xqYMw.jpeg\" width=\"700\" align = \"center\"/>\n",
        "<center>Fig 4. Working</center>\n",
        "<br>\n",
        "\n",
        "First, Let's discuss the traditional approach of neural style transfer first given by Gatys et al. in there paper \"A Neural Algorithm of Artistic Style\".It was built on a very neat idea that,\n",
        "\n",
        "    It is possible to separate the style representation and content representations in a CNN, learnt during a computer vision task (e.g. image recognition task).\n",
        "\n",
        "Neural style transfer uses a pretrained convolution neural network. Then to define a loss function which blends two images seamlessly to create visually appealing art, NST defines the following inputs:\n",
        "\n",
        "    1. A content image (c) ‚Äî the image we want to transfer a style to\n",
        "    2. A style image (s) ‚Äî the image we want to transfer the style from\n",
        "    3. An input (generated) image (g) ‚Äî the image that contains the final result (the only trainable variable)\n",
        "\n",
        "The basic idea behind this approach is that CNN pretrained on large image datasets develop an intuition of how images and objects in those images look in terms of content and style. The shallow layers of these networks are more concerned with content of the image like shapes and structural details. The deeper layers are good at understanding the texture and style of the image.\n",
        "<br>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*EvBcni8o_O3v4RUl640TZQ@2x.png\" width=\"700\" align = \"center\"/>\n",
        "<center>Fig 5. Features extracted at different levels</center>\n",
        "<br>\n",
        "\n",
        "So, content, style and generated images are passed through the network and the weigts of specific layers are compared using loss fuctions like content loss and style loss.\n",
        "<br>\n",
        "\n",
        "**Content Loss**: The content cost function is making sure that the content present in the content image is captured in the generated image. As CNNs capture information about content in the higher levels, where the lower levels are more focused on individual pixel values, we use the top-most CNN layer to define the content loss function.\n",
        "<br>\n",
        "\n",
        "**Style Loss**:To extract the style information from the VGG network, we use all the layers of the CNN. Furthermore, style information is measured as the amount of correlation present between features maps in a given layer. Next, a loss is defined as the difference of correlation present between the feature maps computed by the generated image and the style image.\n",
        "<br>\n",
        "\n",
        "Then, an optimizer back-propagates and updates the pixel values of the generated image and the process repeats. This process of searching for pixel values is very slow and not at all practical for styling multiple images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEa_cMiSV9Kq"
      },
      "source": [
        "## The Problem:\n",
        "Each new content image will reset the generated image pixels and the process of pixel search needs to be done again. That makes the process very very slow and does not gurantee good results. Due these time and compute constraints, it cannot be implemented in production.\n",
        "\n",
        "\n",
        "## The Solution:\n",
        "The solution is to generalize the approach, using something like a neural net that learns to apply a specific type of style on any input image. Although this approach is also not very good but it is much better than the previous one.\n",
        "\n",
        "**Advantages**:\n",
        "* Much faster than the traditional approach\n",
        "* requires us to train the model only once per style\n",
        "\n",
        "**Disadvantages**:\n",
        "* Each style requires its own weights for the model which means it requires a lot of space to save weights for each type of style."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPcYu3QhV9Kr"
      },
      "source": [
        "## Fast Neural Style Transfer\n",
        "\n",
        "<img src=\"https://www.fritz.ai/images/fast_style_transfer_arch.jpg\" width=\"700\" align = \"center\"/>\n",
        "<center>Fig 6. Working of TransformerNet and VGG16 for fast NST</center>\n",
        "<br>\n",
        "\n",
        "Training a style transfer model requires two networks: a pre-trained feature extractor and a transfer network. The pre-trained feature extractor is used to avoid having to us paired training data. It‚Äôs usefulness arises from the curious tendency for individual layers of deep convolutional neural networks trained for image classification to specialize in understanding specific features of an image.\n",
        "<br>\n",
        "\n",
        "The pre-trained model enables us to compare the content and style of two images, but it doesn't actually help us create the stylized image. That‚Äôs the job of a second neural network, which we‚Äôll call the transfer network. The transfer network is an image translation network that takes one image as input and outputs another image. Transfer networks typically have an encode-decoder architecture.\n",
        "<br>\n",
        "\n",
        "At the beginning of training, one or more style images are run through the pre-trained feature extractor, and the outputs at various style layers are saved for later comparison. Content images are then fed into the system. Each content image passes through the pre-trained feature extractor, where outputs at various content layers are saved. The content image then passes through the transfer network, which outputs a stylized image. The stylized image is also run through the feature extractor, and outputs at both the content and style layers are saved.\n",
        "<br>\n",
        "\n",
        "The quality of the stylized image is defined by a custom loss function that has terms for both content and style. The extracted content features of the stylized image are compared to the original content image, while the extracted style features are compared to those from the reference style image(s). After each step, only the transfer network is updated. The weights of the pre-trained feature extractor remain fixed throughout. By weighting the different terms of the loss function, we can train models to produce output images with lighter or heavier stylization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flijqT8cV9Kr"
      },
      "source": [
        "## Requirements:\n",
        "For smooth working of this notebook please use these settings. <br>\n",
        "Create a new virtual environment and install these dependencies in it.\n",
        "1. Python == 3.7.6\n",
        "2. Torch == 1.5.1\n",
        "3. Torchvision == 0.6.0a0+35d732a\n",
        "4. Numpy == 1.18.1\n",
        "5. PIL == 5.4.1\n",
        "6. tqdm == 4.45.0\n",
        "7. Matplotlib == 3.2.1\n",
        "8. OpenCV == 4.2.0.34\n",
        "9. CUDA Version == 10.1\n",
        "\n",
        "## Usage:\n",
        "Run the **fast_trainer** function to train your custom model or use the provided pretrained model with the test function, **test_image**, to generate results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeuZQkUUV9Kr"
      },
      "source": [
        "### Imports and Setup\n",
        "Let's download all the required files and import all modules."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cellule 1: DRIVE + CONFIGS (CORRIG√âE - AUCUN DATASET SUR DRIVE)\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "STYLE_NAME = \"mon_style\"  # üëà CHANGE √áA √Ä CHAQUE STYLE\n",
        "STYLE_DIR = f\"/content/drive/MyDrive/{STYLE_NAME}\"\n",
        "os.makedirs(STYLE_DIR, exist_ok=True)\n",
        "\n",
        "CONFIGS = [\n",
        "    {\"style_weight\": 1e10,  \"content_weight\": 1e5,   \"pool_type\": \"maxpool\", \"config_id\": \"config1\"},\n",
        "    {\"style_weight\": 10e10, \"content_weight\": 10e3,  \"pool_type\": \"maxpool\", \"config_id\": \"config2\"},\n",
        "    {\"style_weight\": 10e10, \"content_weight\": 10e5,  \"pool_type\": \"maxpool\", \"config_id\": \"config3\"},\n",
        "    {\"style_weight\": 10e10, \"content_weight\": 10e5,  \"pool_type\": \"avgpool\", \"config_id\": \"config4\"},\n",
        "    {\"style_weight\": 10e20, \"content_weight\": 10e3,  \"pool_type\": \"maxpool\", \"config_id\": \"config5\"}\n",
        "]\n",
        "CONFIG_ID = \"config1\"  # üëà CHANGE config1‚Üí2‚Üí3‚Üí4‚Üí5\n",
        "config = next(c for c in CONFIGS if c[\"config_id\"] == CONFIG_ID)\n",
        "print(f\"‚úÖ Config: {CONFIG_ID}\")\n",
        "print(f\"   style_w={config['style_weight']:.0e} | content_w={config['content_weight']:.0e} | pool={config['pool_type']}\")\n",
        "print(f\"üíæ MOD√àLES ‚Üí {STYLE_DIR}\")\n",
        "print(f\"üìÅ DATASET ‚Üí /content/ (TEMPORAIRE Colab seulement)\")\n",
        "print(f\"üö´ RIEN sur Drive sauf .pth !\")\n"
      ],
      "metadata": {
        "id": "LhNidASJONB5",
        "outputId": "a5bb3e39-0479-4bd1-884b-de20d9667af0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1023786063.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mSTYLE_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mon_style\"\u001b[0m  \u001b[0;31m# üëà CHANGE √áA √Ä CHAQUE STYLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mSTYLE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/drive/MyDrive/{STYLE_NAME}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTYLE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m CONFIGS = [\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XRrpahNLV9Ks",
        "outputId": "ec235ec5-b023-4da2-f2e2-46e8a8f68c0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-30 16:07:10--  http://images.cocodataset.org/zips/test2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.28.95, 16.15.207.32, 54.231.171.57, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.28.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6646970404 (6.2G) [application/zip]\n",
            "Saving to: ‚Äòtest2017.zip‚Äô\n",
            "\n",
            "test2017.zip        100%[===================>]   6.19G  53.1MB/s    in 2m 6s   \n",
            "\n",
            "2025-11-30 16:09:17 (50.2 MB/s) - ‚Äòtest2017.zip‚Äô saved [6646970404/6646970404]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Uncomment and Download data for training (6.1 GB) \"\"\"\n",
        "!wget http://images.cocodataset.org/zips/test2017.zip\n",
        "!mkdir './dataset'\n",
        "!unzip -q ./test2017.zip -d './dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ENSWSVPV9Kt",
        "outputId": "291091b4-3a11-4a9e-d37c-7d8ab3798712",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Download the best model weights \"\"\"\n",
        "!mkdir ./checkpoints\n",
        "!wget -q -O 'best_model.pth' https://www.dropbox.com/s/7xvmmbn1bx94exz/best_model.pth?dl=1\n",
        "!mv best_model.pth ./checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekFa6Z-xzMZs"
      },
      "outputs": [],
      "source": [
        "\"\"\" Download content and style images \"\"\"\n",
        "!mkdir ./content\n",
        "!mkdir ./style\n",
        "!wget -q https://github.com/myelinfoundry-2019/challenge/raw/master/japanese_garden.jpg -P './content'\n",
        "!wget -q https://github.com/myelinfoundry-2019/challenge/raw/master/picasso_selfportrait.jpg -P './style'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrt4hIC3uy3O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from collections import namedtuple\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from PIL import Image\n",
        "import glob\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42) #for reproducibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mean and standard deviation used for training\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwRPQ8XDu4Ls"
      },
      "source": [
        "## Defining Models\n",
        "Here we have 2 models\n",
        "1. **VGG16**: Pre-trained model for feature extraction for loss comparisions.\n",
        "2. **TransformerNet**: The main model which acts as an encoder-decoder pair and learns to convert any image to a specific style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlyzsaqPu1kl"
      },
      "outputs": [],
      "source": [
        "\"\"\" Pretrained VGG16 Model \"\"\"\n",
        "class VGG16(torch.nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(VGG16, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
        "        # üî• Lecture de pool_type\n",
        "        if config['pool_type'] == \"avgpool\":\n",
        "            for i, layer in enumerate(vgg_pretrained_features):\n",
        "                if isinstance(layer, nn.MaxPool2d):\n",
        "                    vgg_pretrained_features[i] = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        elif config['pool_type'] == \"maxpool\":\n",
        "            pass  # On ne change rien\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"pool_type inconnu : {pool_type}\")\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "\n",
        "\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = self.slice1(X)\n",
        "        h_relu1_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4_3 = h\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n",
        "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
        "        return out\n",
        "\n",
        "\n",
        "\"\"\" Transformer Net \"\"\"\n",
        "class TransformerNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            ConvBlock(3, 32, kernel_size=9, stride=1),\n",
        "            ConvBlock(32, 64, kernel_size=3, stride=2),\n",
        "            ConvBlock(64, 128, kernel_size=3, stride=2),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ConvBlock(128, 64, kernel_size=3, upsample=True),\n",
        "            ConvBlock(64, 32, kernel_size=3, upsample=True),\n",
        "            ConvBlock(32, 3, kernel_size=9, stride=1, normalize=False, relu=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\"\"\" Components of Transformer Net \"\"\"\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=True),\n",
        "            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x) + x\n",
        "\n",
        "\n",
        "class ConvBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, upsample=False, normalize=True, relu=True):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(kernel_size // 2), nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "        )\n",
        "        self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None\n",
        "        self.relu = relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            x = F.interpolate(x, scale_factor=2)\n",
        "        x = self.block(x)\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        if self.relu:\n",
        "            x = F.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGUiIXCvvIt0"
      },
      "source": [
        "## Utility functions\n",
        "These functions help in the training process from preprocessing the input image to calculating the gram-matrix for loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqnLDfJ5u7YY"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(y):\n",
        "    \"\"\" Returns the gram matrix of y (used to compute style loss) \"\"\"\n",
        "    (b, c, h, w) = y.size()\n",
        "    features = y.view(b, c, w * h)\n",
        "    features_t = features.transpose(1, 2)\n",
        "    gram = features.bmm(features_t) / (c * h * w)\n",
        "    return gram\n",
        "\n",
        "\n",
        "def train_transform(image_size):\n",
        "    \"\"\" Transforms for training images \"\"\"\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((int(image_size * 1.15),int(image_size * 1.15))),\n",
        "            transforms.RandomCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ]\n",
        "    )\n",
        "    return transform\n",
        "\n",
        "\n",
        "def style_transform(image_size=None):\n",
        "    \"\"\" Transforms for style image \"\"\"\n",
        "    resize = [transforms.Resize((image_size,image_size))] if image_size else []\n",
        "    transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "    return transform\n",
        "\n",
        "def test_transform(image_size=None):\n",
        "    \"\"\" Transforms for test image \"\"\"\n",
        "    resize = [transforms.Resize(image_size)] if image_size else []\n",
        "    transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "    return transform\n",
        "\n",
        "def denormalize(tensors):\n",
        "    \"\"\" Denormalizes image tensors using mean and std \"\"\"\n",
        "    for c in range(3):\n",
        "        tensors[:, c].mul_(std[c]).add_(mean[c])\n",
        "    return tensors\n",
        "\n",
        "\n",
        "def deprocess(image_tensor):\n",
        "    \"\"\" Denormalizes and rescales image tensor \"\"\"\n",
        "    image_tensor = denormalize(image_tensor)[0]\n",
        "    image_tensor *= 255\n",
        "    image_np = torch.clamp(image_tensor, 0, 255).cpu().numpy().astype(np.uint8)\n",
        "    image_np = image_np.transpose(1, 2, 0)\n",
        "    return image_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqi1dkAcV9Kv"
      },
      "source": [
        "## Training Loop\n",
        "This is our main training loop. Here we follow a specific order of steps to train our neural net. The steps are as follows:\n",
        "1. First, the train dataloaders are initialized to provide us with the batches of data that the model will use to train on.\n",
        "2. Then the neural nets are initialized for usage.\n",
        "3. After that we initialize optimizer which will update the weights of the model and help in training. The optimizer takes a very important hyperparameter called **learning rate** which defines how intensly model weights are updated. A good learning rate marks the balance between slow training and overshooting.\n",
        "4. Next, we transform our input images to desired shape and keep a small set of 8 images aside for validation purpose. These 8 images are used to understand how the model training progresses.\n",
        "5. After this, the main process starts. The outer loop runs \"epochs\" number of times. The inner loop iterates over the batches provided by the dataloader. Model output is generated for the input image, loss is calculated for the whole batch and model weights are updated using back-propogation. All this runs multiple times in each epoch.\n",
        "6. During the training we keep saving the model weights and ouput of the model on the validation set we kept aside earlier.\n",
        "\n",
        "Please find all my experiments and there outputs [here.](https://drive.google.com/drive/folders/13jTfhQVB2qojOD3cb9EF7-Uy_afYUbDE?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1NG_ZHqvVFU",
        "outputId": "c5293688-a3ba-4355-b0c5-9c8451be8367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'config' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1508490591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                  \u001b[0mcheckpoint_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                  \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                  \u001b[0mlambda_style\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                  lambda_content=config['content_weight'],):\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ],
      "source": [
        "def fast_trainer(style_image,\n",
        "                 style_name,\n",
        "                 dataset_path,\n",
        "                 image_size=256,\n",
        "                 style_size=448,\n",
        "                 batch_size = 8,\n",
        "                 lr = 1e-5,\n",
        "                 epochs = 1,\n",
        "                 checkpoint_model = None,\n",
        "                 checkpoint_interval=500,\n",
        "                 sample_interval=200,\n",
        "                 lambda_style=config['style_weight'],\n",
        "                 lambda_content=config['content_weight'],):\n",
        "\n",
        "    os.makedirs(f\"./images/outputs/{style_name}-training\", exist_ok=True)\n",
        "    os.makedirs(f\"./checkpoints\", exist_ok=True)\n",
        "\n",
        "\n",
        "    \"\"\" Create dataloader for the training data \"\"\"\n",
        "    train_dataset = datasets.ImageFolder(dataset_path, train_transform(image_size))\n",
        "    dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    \"\"\" Define networks \"\"\"\n",
        "    transformer = TransformerNet().to(device)\n",
        "    vgg = VGG16(requires_grad=False).to(device)\n",
        "\n",
        "    \"\"\" Load checkpoint model if specified \"\"\"\n",
        "    if checkpoint_model:\n",
        "        transformer.load_state_dict(torch.load(checkpoint_model))\n",
        "\n",
        "    \"\"\" Define optimizer and loss \"\"\"\n",
        "    optimizer = Adam(transformer.parameters(), lr)\n",
        "    l2_loss = torch.nn.MSELoss().to(device)\n",
        "\n",
        "    \"\"\" Load style image \"\"\"\n",
        "    style = style_transform(style_size)(Image.open(style_image))\n",
        "    style = style.repeat(batch_size, 1, 1, 1).to(device)\n",
        "\n",
        "    \"\"\" Extract style features \"\"\"\n",
        "    features_style = vgg(style)\n",
        "    gram_style = [gram_matrix(y) for y in features_style]\n",
        "\n",
        "    \"\"\" Sample 8 images for visual evaluation of the model \"\"\"\n",
        "    image_samples = []\n",
        "    for path in random.sample(glob.glob(f\"{dataset_path}/*/*.jpg\"), 8):\n",
        "        image_samples += [style_transform(image_size)(Image.open(path))]\n",
        "    image_samples = torch.stack(image_samples)\n",
        "\n",
        "    def save_sample(batches_done):\n",
        "        \"\"\" Evaluates the model and saves image samples \"\"\"\n",
        "        transformer.eval()\n",
        "        with torch.no_grad():\n",
        "            output = transformer(image_samples.to(device))\n",
        "        image_grid = denormalize(torch.cat((image_samples.cpu(), output.cpu()), 2))\n",
        "        save_image(image_grid, f\"./images/outputs/{style_name}-training/{batches_done}.jpg\", nrow=4)\n",
        "        transformer.train()\n",
        "\n",
        "\n",
        "    train_metrics = {\"content\": [], \"style\": [], \"total\": []}\n",
        "    for epoch in range(epochs):\n",
        "        epoch_metrics = {\"content\": [], \"style\": [], \"total\": []}\n",
        "        for batch_i, (images, _) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            images_original = images.to(device)\n",
        "            images_transformed = transformer(images_original)\n",
        "\n",
        "            # Extract features\n",
        "            features_original = vgg(images_original)\n",
        "            features_transformed = vgg(images_transformed)\n",
        "\n",
        "            # Compute content loss as MSE between features\n",
        "            content_loss = lambda_content * l2_loss(features_transformed.relu2_2, features_original.relu2_2)\n",
        "\n",
        "            # Compute style loss as MSE between gram matrices\n",
        "            style_loss = 0\n",
        "            for ft_y, gm_s in zip(features_transformed, gram_style):\n",
        "                gm_y = gram_matrix(ft_y)\n",
        "                style_loss += l2_loss(gm_y, gm_s[: images.size(0), :, :])\n",
        "            style_loss *= lambda_style\n",
        "\n",
        "            total_loss = content_loss + style_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_metrics[\"content\"] += [content_loss.item()]\n",
        "            epoch_metrics[\"style\"] += [style_loss.item()]\n",
        "            epoch_metrics[\"total\"] += [total_loss.item()]\n",
        "\n",
        "            train_metrics[\"content\"] += [content_loss.item()]\n",
        "            train_metrics[\"style\"] += [style_loss.item()]\n",
        "            train_metrics[\"total\"] += [total_loss.item()]\n",
        "\n",
        "            sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d/%d] [Content: %.2f (%.2f) Style: %.2f (%.2f) Total: %.2f (%.2f)]\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    epochs,\n",
        "                    batch_i,\n",
        "                    len(train_dataset),\n",
        "                    content_loss.item(),\n",
        "                    np.mean(epoch_metrics[\"content\"]),\n",
        "                    style_loss.item(),\n",
        "                    np.mean(epoch_metrics[\"style\"]),\n",
        "                    total_loss.item(),\n",
        "                    np.mean(epoch_metrics[\"total\"]),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            batches_done = epoch * len(dataloader) + batch_i + 1\n",
        "            if batches_done % sample_interval == 0:\n",
        "                save_sample(batches_done)\n",
        "\n",
        "            if checkpoint_interval > 0 and batches_done % checkpoint_interval == 0:\n",
        "                torch.save(transformer.state_dict(), f\"./checkpoints/{style_name}_config{CONFIG_ID}_{batches_done}.pth\")\n",
        "\n",
        "\n",
        "            torch.save(transformer.state_dict(), f\"./checkpoints/{style_name}_config{CONFIG_ID}_last_checkpoint.pth\")\n",
        "\n",
        "    print(\"Training Completed!\")\n",
        "\n",
        "    #printing the loss curve.\n",
        "    plt.plot(train_metrics[\"content\"], label = \"Content Loss\")\n",
        "    plt.plot(train_metrics[\"style\"], label = \"Style Loss\")\n",
        "    plt.plot(train_metrics[\"total\"], label = \"Total Loss\")\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFsM7BA5V9Kw"
      },
      "source": [
        "## Testing and Inference Loop\n",
        "After the model has been trained it can be used to generate outputs for desired inputs. Each model is trained on a single style and can produce images with that single style. That means we require multiple model, one model per style, if we want to use this in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQAntmdV8l1V"
      },
      "outputs": [],
      "source": [
        "def test_image(image_path,checkpoint_model,save_path):\n",
        "    os.makedirs(os.path.join(save_path,\"results\"), exist_ok=True)\n",
        "\n",
        "    transform = test_transform()\n",
        "\n",
        "    # Define model and load model checkpoint\n",
        "    transformer = TransformerNet().to(device)\n",
        "    transformer.load_state_dict(torch.load(checkpoint_model))\n",
        "    transformer.eval()\n",
        "\n",
        "    # Prepare input\n",
        "    image_tensor = Variable(transform(Image.open(image_path))).to(device)\n",
        "    image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "    # Stylize image\n",
        "    with torch.no_grad():\n",
        "        stylized_image = denormalize(transformer(image_tensor)).cpu()\n",
        "    # Save image\n",
        "    fn = checkpoint_model.split('/')[-1].split('.')[0]\n",
        "    save_image(stylized_image, os.path.join(save_path,f\"results/{fn}-output.jpg\"))\n",
        "    print(\"Image Saved!\")\n",
        "    plt.imshow(cv2.cvtColor(cv2.imread(os.path.join(save_path,f\"results/{fn}-output.jpg\")), cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0HJVAD-V9Kw"
      },
      "source": [
        "### To train run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9H9uAcb2ugb",
        "outputId": "6add1f90-7c9d-4f11-9518-2e23f6bad27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/1] [Batch 199/40670] [Content: 710321.50 (787775.36) Style: 1658286.50 (3582549.15) Total: 2368608.00 (4370324.50)]"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './images/outputs/Futurist_simple_configconfig1-training/200.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2194895529.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#/content/drive/MyDrive/FNST_styles/Checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m fast_trainer(style_image='/content/drive/MyDrive/FNST_styles/styles/style_bd_futuriste_simple.png',style_name = 'Futurist_simple',\n\u001b[0m\u001b[1;32m     16\u001b[0m              dataset_path='./dataset/', epochs = 1)\n",
            "\u001b[0;32m/tmp/ipython-input-1062843860.py\u001b[0m in \u001b[0;36mfast_trainer\u001b[0;34m(style_image, style_name, dataset_path, image_size, style_size, batch_size, lr, epochs, checkpoint_model, checkpoint_interval, sample_interval, lambda_style, lambda_content)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mbatches_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatches_done\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0msave_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_interval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatches_done\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcheckpoint_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1062843860.py\u001b[0m in \u001b[0;36msave_sample\u001b[0;34m(batches_done)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mimage_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"./images/outputs/{style_name}_config{CONFIG_ID}-training/{batches_done}.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './images/outputs/Futurist_simple_configconfig1-training/200.jpg'"
          ]
        }
      ],
      "source": [
        "\"\"\" Run this to train the model \"\"\"\n",
        "#[NOTE]: For representation purpose i am using a smaller dataset. Pls use the dataset given at the start of this notebook\n",
        "#for better results and change the dataset_path in this function.\n",
        "#./style/picasso_selfportrait.jpg\n",
        "#/content/drive/MyDrive/FNST_styles/styles/style_bd_futuriste_classique.png\n",
        "#/content/drive/MyDrive/FNST_styles/styles/style_bd_futuriste_dark.png\n",
        "#/content/drive/MyDrive/FNST_styles/styles/style_bd_futuriste_manga.png\n",
        "#/content/drive/MyDrive/FNST_styles/styles/style_bd_futuriste_realiste.png\n",
        "#/content/drive/MyDrive/FNST_styles/styles/style_bd_futuriste_selfie.png\n",
        "#/content/drive/MyDrive/FNST_styles/styles/style_bd_futuriste_simple.png\n",
        "#/content/drive/MyDrive/FNST_styles/styles/style_bd_futuriste_startrek.png\n",
        "\n",
        "\n",
        "#/content/drive/MyDrive/FNST_styles/Checkpoints\n",
        "fast_trainer(style_image='/content/drive/MyDrive/FNST_styles/styles/style_bd_futuriste_simple.png',style_name = 'Futurist_simple',\n",
        "             dataset_path='./dataset/', epochs = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7FyqcNuV9Kx"
      },
      "source": [
        "### To test on your own image run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HwYW0qNjyL5"
      },
      "outputs": [],
      "source": [
        "test_image(image_path = './content/japanese_garden.jpg',\n",
        "           checkpoint_model = './checkpoints/best_model.pth',\n",
        "           save_path = './')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5awht88V9Kx"
      },
      "source": [
        "## Experiments\n",
        "I experimented with different layer formats and style and content weights and these are the results of each experiment.\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/ikaq1w8ywmurbk1/Screenshot_2020-07-04%20yash-choudhary%20Neural-Style-Transfer.png?raw=1\" width=\"500\" align = \"center\"/>\n",
        "<center>Table 1. Experiments</center>\n",
        "<br>\n",
        "<br>\n",
        "Now let's look the results of each experiment at different instances.\n",
        "<br>\n",
        "<br>\n",
        "<img src=\"https://www.dropbox.com/s/7plcfdag664z5k5/grid.png?raw=1\" width=\"900\" align = \"center\"/>\n",
        "<center>Fig 7. Best Result 1 [More Weight to Style]</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj1gdpwcV9Kx"
      },
      "source": [
        "## Result\n",
        "The 3 best outputs from my models are:\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/8a7i1qufrn2th8i/best_output1.jpg?raw=1\" width=\"500\" align = \"center\"/>\n",
        "<center>Fig 8. Best Result 1 [More Weight to Style]</center>\n",
        "<br>\n",
        "<img src=\"https://www.dropbox.com/s/mmyikx154whtufj/best_output2.jpg?raw=1\" width=\"500\" align = \"center\"/>\n",
        "<center>Fig 9. Best Result 2 [Balanced Style and content]</center>\n",
        "<br>\n",
        "<img src=\"https://www.dropbox.com/s/h7nrahjbek3ajq1/best_output3.jpg?raw=1\" width=\"500\" align = \"center\"/>\n",
        "<center>Fig 10. Best Result 3 [More Weight to Content]</center>\n",
        "\n",
        "Please find detailed experiment results [here.](https://drive.google.com/drive/folders/13jTfhQVB2qojOD3cb9EF7-Uy_afYUbDE?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebl8KyMjV9Kx"
      },
      "source": [
        "### Important Links\n",
        "1. Train Dataset Link: http://images.cocodataset.org/zips/test2017.zip <br>\n",
        "2. Style Image: https://github.com/myelinfoundry-2019/challenge/raw/master/picasso_selfportrait.jpg <br>\n",
        "3. Content Image: https://github.com/myelinfoundry-2019/challenge/raw/master/japanese_garden.jpg <br>\n",
        "4. Best Model: https://www.dropbox.com/s/7xvmmbn1bx94exz/best_model.pth?dl=1\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### References:\n",
        "1. [Style Transfer Guide](https://www.fritz.ai/style-transfer/)\n",
        "2. [Breaking Down Leon Gatys‚Äô Neural Style Transfer in PyTorch](https://towardsdatascience.com/breaking-down-leon-gatys-neural-style-transfer-in-pytorch-faf9f0eb79db)\n",
        "3. [Intuitive Guide to Neural Style Transfer](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee)\n",
        "4. [A Neural Algorithm of Artistic Style By\n",
        "Leon A. Gatys, Alexander S. Ecker, Matthias Bethge](https://arxiv.org/abs/1508.06576)\n",
        "5. [Perceptual Losses for Real-Time Style Transfer and Super-Resolution by Justin Johnson, Alexandre Alahi, Li Fei-Fei](https://arxiv.org/abs/1603.08155)\n",
        "6. [Neural Style Transfer on Real Time Video (With Full implementable code)](https://towardsdatascience.com/neural-style-transfer-on-real-time-video-with-full-implementable-code-ac2dbc0e9822)\n",
        "7. [Classic Neural Style Transfer](https://github.com/halahup/NeuralStyleTransfer)\n",
        "8. [Fast Neural Style Transfer using Lua](https://github.com/lengstrom/fast-style-transfer)\n",
        "9. [Fast Neural Style Transfer using Python](https://github.com/eriklindernoren/Fast-Neural-Style-Transfer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}